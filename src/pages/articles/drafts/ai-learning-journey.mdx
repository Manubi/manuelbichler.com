import { ArticleLayout } from '@/components/ArticleLayout'

export const meta = {
  author: 'Manuel Bichler',
  date: '2023-01-12',
  title: 'My AI learning journey',
  description:
    'Summary of my learning journey of Deep learning beginning january 2023.',
}

export default (props) => <ArticleLayout meta={meta} {...props} />

# My AI learning journey

## Why

By know everyone in tech knows that AI will most knowledgework workflows. Meaning what the wordprocessor did a couple decades ago will happen again but just much faster. So in order to get an idea
whats possible I decided to learn about AI. To be more precise I want to learn more about Deep Larning. Why? Because this is currently the most promising method. I will document my journey here.

## plan

I will start with the fast.ai course and then go on with the book "Deep Learning" by Ian Goodfellow and Yoshua Bengio and Aaron Courville. I will also try to do some projects on my own.
The main goal after the course of fast.ai is to bring dog noses on the blockchain. Might sound stupid, but hey life is short.

After that I want to add an LLM search to our work project Resolute.

## Resources

- [fast.ai](https://www.fast.ai/)
- [Deep Learning Book](http://www.deeplearningbook.org/)

## vocabulary

As with learning every knew field there is a ton of new vocabulary I need to get used to. I will try to collect it here.

the functional form of a model: architecture - It's the mathematical function that maps the input to the output. In deep learning, this function is typically implemented using artificial neural networks, which are composed of layers of interconnected nodes (neurons) that perform a series of mathematical operations on the input data to produce the output.
the weights: parameters - The functional form of a neural network can be represented by a set of parameters (also called weights) that are learned through a process of training using a set of input-output pairs. During training, the network adjusts the weights to minimize a cost function that measures the difference between the predicted output and the true output. Once the network has been trained, it can be used to make predictions on new input data by applying the learned function to the input.
the results: predictions - in deep learning are the outputs generated by a trained model when given a new set of input data. In other words, the model predicts what the output should be for a given input based on the patterns it learned during the training process.
the measure of performance: loss - The measure of performance is a metric that is used to evaluate the performance of a model. The goal of training a model is to find a set of weights that minimize the cost function. The cost function is a measure of how well the model fits the training data. The measure of performance is a metric that is used to evaluate the performance of a model. The goal of training a model is to find a set of weights that minimize the cost function. The cost function is a measure of how well the model fits the training data.
the dependent variable: targets - The dependent variable is the variable that is being predicted by the model. In other words, the dependent variable is the output of the model. The dependent variable is the variable that is being predicted by the model. In other words, the dependent variable is the output of the model.

We need to turn our downloaded data into `DataLoaders` object we need to tell fastai at least four things:

- What kinds of data we are working with (images, text, tabular, etc.)
- Where to find the data (in files, in a database, in a cloud storage bucket, etc.)
- How to label these items
- How to create a validation set from our training set

```python
    bears = DataBlock(
    blocks=(ImageBlock, CategoryBlock),
    get_items=get_image_files,
    splitter=RandomSplitter(valid_pct=0.2, seed=42),
    get_y=parent_label,
    item_tfms=Resize(128))
```

First we provide a tuble where we specify what types we want for the independent and dependent variables.
`blocks=(ImageBlock, CategoryBlock)`

intependent variable: thing were using to make predictions from.
dependent variable: thing we want to predict - our target.
in this case independent = image, dependent = category (type of bear) for each image

`get_images=get_iamge_files`: get_image_files is a function takes a path and returns a list of all the files in that path.

Then we tell fastai how to get the items. In this case we use the function `get_image_files` which will return all the files in the folder we pass to it.

`splitter=RandomSplitter(valid_pct=0.2, seed=42)`: we need to tell fastai how to split the data into a training and validation set. We use the `RandomSplitter` function which will randomly select 20% of the data for the validation set. We also pass a seed so that we get the same validation set each time we run the code.

`get_y=parent_label`: we need to tell fastai how to get the labels for each item. In this case we use the `parent_label` function which will return the name of the folder that the item is in.

The images are most often in different sizes. This is a problem for deep learning. We don't feed the model one image at a time but several of them. They are called mini-batches. We also need to group them
in a big array (usally called a tensor) that is going to go through our model. All in the same size. The `item_tfms=Resize(128)` will resize all the images to 128x128 pixels.

by know we have a `DataBlock` object. That's like a template for creating `DataLoaders`. A single `DataLoader` is a class that provides patches of a few items at at time to the GPU. Fastai will give you 64 items at a time. All stacked up into a single tensor.
To look them up we can use the `show_batch` method.
`dls = bears.dataloaders(path)`

`dls.valid.show_batch(max_n=4, nrows=1)`

By default `Resize` crops the images to fit a square. This means we sometimes can loose important deatils.
We can change that by passing `Resize(128, ResizeMethod.Squish)`. This will resize the images to 128x128 pixels but will not crop them.

`bears = bears.new(item_tfms=RandomResizedCrop(128, ResizeMethod.Squish))`
`dls = bears.dataloaders(path)`
`dls.valid.show_batch(max_n=4, nrows=1)`

Another example would be to resize and crop them but do it randomly. Meaning we always feed the model a different part of the same image.
`bears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))`
`dls = bears.dataloaders(path)`
`dls.train.show_batch(max_n=4, nrwos=1, unique=True)`

`unique=True` will make sure that we don't see the same image twice.

Data augmentation refers to creating random variations of our input data, such that they appear different, but do not actually change the meaning of the data.
f.e. image roation, flipping, perspective warping, brightness and contrast changes, etc.
Because our images are alle the same size already, we can apply these augumentations to an entire batch of them using the GPU, which will save a lot of time.
To tell fastai we want these transforms on a batch, we use the `batch_tfms` argument.
`bears = bears.new(item_tfms=RandomResizedCrop(128), batch_tfms=aug_transforms(mult=2))`
`dls = bears.dataloaders(path)`
`dls.train.show_batch(max_n=8, nrwos=2, unique=True)`

Now that we have assembled our data in a format fit for model training, let's actually train an image classifier using it.

As we don't have a lot of data (150 images) we will use RandomResizedCropwith an image size of 224px which is fairly standard for image classification, and a default aug_transforms.first-letter:first-line:

```python
    bears = bears.new(
        item_tfms=RandomResizedCrop(224, min_scale=0.5),
        batch_tfms=aug_transforms())
    dls = bears.dataloaders(path)
```

After that we can create our `Learner` and finetune it the usual way.

```python
    learn = vision_leanrer(dls, resnet18, metrics=error_rate)
    learn.fine_tune(4)
```

After leanring we create a Confusion matrix to see how well our model is doing.

```python
    interp = ClassificationInterpretation.from_learner(learn)
    interp.plot_confusion_matrix()
```

To show the top losses in the model we can use the `plot_top_losses` method.

```python
    interp.plot_top_losses(5, nrows=1)
```

The intuitive approach to do data cleaningis to do it before you train a model. But as you'll see in many cases, a model can actually help you find data issues more quickly and easily.
So, we normally prefer to train a quick and simple model first, and then use it to help us clean our data.

fastai has a handy GUI for doing that. `ImageClassifierCleaner` will show you the images that your model is most confused about, and let you easily remove them from your dataset.

```python
    from fastai.vision.widgets import *
    cleaner = ImageClassifierCleaner(learn)
    cleaner
```

We can then use the `clean` method to remove the images that we don't want to use. The cleaner only returns the indices of the images we want to keep/delete, so we can use that to filter our dataset.

```python
    for idx in cleaner.delete(): cleaner.fns[idx].unlink() # delete the file
    for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat) # move the file to the new category
```

Cleaning the data and getting it ready for your model are two of the biggest challenges for data scientists. They say it takes 90% of the time. But with fastai, you can do it in just a few lines of code.

## Exporting your model

once you're happy with your model you need to export it. A model consists of two parts: the architecture and the weights/parameters. The easiest way is to save both of these.

```python
    learn.export() #generates a export.pkl file
```

When we using a model for getting predictions, instead of training, we call it inference. To create our inference learner from the exported file, we use

## 03 ethics
